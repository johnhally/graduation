import json
from datetime import datetime
from typing import List, Dict, Any
from agentic_memory.memory_system import AgenticMemorySystem


class MemoryChatBot:
    def __init__(self, memory_system: AgenticMemorySystem, max_history: int = 10):
        self.memory_system = memory_system
        self.history = []  # çŸ­æœŸå¯¹è¯å†å² (Short-term memory)
        self.max_history = max_history  # é™åˆ¶ä¸Šä¸‹æ–‡è½®æ•°ï¼Œé˜²æ­¢tokenæº¢å‡º

        # å®šä¹‰æœºå™¨äººçš„â€œäººè®¾â€å’ŒæŒ‡ä»¤
        self.system_prompt = """
        You are a personalized AI assistant with access to the user's long-term memory.
        
        Your Goal:
        Answer the user's questions or engage in chat by utilizing the [Relevant Memories] provided below.

        Guidelines:
        1. IF the user asks about past events, preferences, or notes, PRIORTIZE information from [Relevant Memories].
        2. IF [Relevant Memories] contains the answer, explicitly mention that you remember it (e.g., "Based on your notes...", "I recall that...").
        3. IF the information is not in the memories, simply say you don't have that information or answer using general knowledge, but clarify it's general knowledge.
        4. Maintain a helpful, empathetic, and clear tone.
        5. Combine multiple memory fragments to form a coherent answer if needed.
        """

    def _get_time_context(self) -> str:
        """è·å–å½“å‰æ—¶é—´å¹¶æ ¼å¼åŒ–ï¼Œç”¨äº Prompt"""
        now = datetime.now()
        weekday = now.strftime("%A")  # æ˜ŸæœŸå‡ 
        date_str = now.strftime("%Y-%m-%d %H:%M")
        return f"Current System Time: {date_str} ({weekday})"

    def _format_memories_for_prompt(self, relevant_memories: List[Dict[str, Any]]) -> str:
        """å°†æ£€ç´¢åˆ°çš„è®°å¿†æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ŒåµŒå…¥ Prompt"""
        if not relevant_memories:
            return "No relevant past memories found."

        formatted_text = "Found the following relevant memories:\n"
        for i, mem in enumerate(relevant_memories):
            # ä½ çš„ search æ–¹æ³•è¿”å›åŒ…å« content, timestamp, tags ç­‰å­—æ®µçš„å­—å…¸
            formatted_text += (
            f"- MEMORY_ID: {mem['id']}\n"
            f"  TIME: {mem.get('timestamp','Unknown')}\n"
            f"  SCORE: {mem.get('score',0):.3f}\n"
            f"  CONTEXT: {mem.get('context','')}\n"
            f"  CONTENT: {mem.get('content','')}\n"
            f"  TAGS: {', '.join(mem.get('tags',[]))}\n"
        )
            print(formatted_text)

        return formatted_text

    def chat(self, user_input: str) -> str:
        """æ ¸å¿ƒå¯¹è¯å¾ªç¯"""

        # --- æ­¥éª¤ 1: æ£€ç´¢è®°å¿† (RAG Core) ---
        # ç®€å•ç­–ç•¥ï¼šç›´æ¥ç”¨ç”¨æˆ·è¾“å…¥å»æœç´¢
        # è¿›é˜¶ç­–ç•¥ï¼ˆæ¯•è®¾åŠ åˆ†ç‚¹ï¼‰ï¼šå¯ä»¥ä½¿ç”¨ LLM å…ˆæŠŠ user_input æ”¹å†™æˆæœç´¢å…³é”®è¯å†æœç´¢
        print(f"ğŸ¤– (Searching memories for: '{user_input}')...")
        results = self.memory_system.search(user_input, k=3)  # æ£€ç´¢æœ€ç›¸å…³çš„3æ¡

        memory_context = self._format_memories_for_prompt(results)
        print(f"ğŸ§  (Retrieved Context):\n{memory_context}")

        # --- æ­¥éª¤ 2: æ„å»ºå®Œæ•´çš„ Prompt ---
        # ç»“æ„ï¼šç³»ç»ŸæŒ‡ä»¤ + é•¿æœŸè®°å¿†ä¸Šä¸‹æ–‡ + çŸ­æœŸå¯¹è¯å†å² + å½“å‰è¾“å…¥

        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "system", "content": self._get_time_context()}, #ğŸ˜€æ·»åŠ äº†æ—¶åº
            {"role": "system", "content": f"[Relevant Memories Retrieved from Database]:\n{memory_context}"}
        ]

        # æ·»åŠ çŸ­æœŸå†å² (æœ€è¿‘ N è½®)
        messages.extend(self.history[-self.max_history:])

        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": user_input})

        # --- æ­¥éª¤ 3: è°ƒç”¨ LLM ç”Ÿæˆå›ç­” ---
        # å¤ç”¨ memory_system ä¸­çš„ llm_controllerï¼Œé¿å…é‡å¤å†™è°ƒç”¨é€»è¾‘
        # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ç›´æ¥é€šè¿‡ client è°ƒç”¨ chat completionï¼Œæˆ–è€…ä¿®æ”¹ llm_controller æ”¯æŒ list æ ¼å¼çš„ messages
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ä½ çš„ LLMController å¯ä»¥å¤„ç† raw prompt æˆ–æˆ‘ä»¬éœ€è¦ç¨å¾®é€‚é…ä¸€ä¸‹

        # ç”±äºä½ çš„ LLMController.get_completion å°è£…æ˜¯é’ˆå¯¹ str prompt çš„
        # æˆ‘ä»¬è¿™é‡Œä¸´æ—¶æ‰‹åŠ¨æ‹¼æ¥æˆ string ä¼ ç»™å®ƒï¼Œæˆ–è€…æœ€å¥½ç»™ LLMController åŠ ä¸€ä¸ª chat æ–¹æ³•
        # æ–¹æ¡ˆ Aï¼šæ‹¼æ¥ String (ç®€å•ï¼Œä½†å¯¹äº chat æ¨¡å‹æ•ˆæœä¸å¦‚ list å¥½)
        full_prompt_str = f"{self.system_prompt}\n\n[Relevant Memories]:\n{memory_context}\n\n[Conversation]:\n"
        for msg in self.history[-self.max_history:]:
            full_prompt_str += f"{msg['role']}: {msg['content']}\n"
        full_prompt_str += f"user: {user_input}\nassistant:"

        # è°ƒç”¨ç”Ÿæˆ (è¿™é‡Œå‡è®¾ä½ ä¸æƒ³æ”¹åº•å±‚ä»£ç ï¼Œæˆ‘ä»¬ç”¨ get_completion å¼ºè¡Œç”Ÿæˆï¼Œè™½ç„¶è¿™é€šå¸¸ç”¨äºå•æ¬¡æŒ‡ä»¤)
        # *æ›´å¥½çš„åšæ³•æ˜¯å»ä¿®æ”¹ llm_controller.py å¢åŠ ä¸€ä¸ª chat() æ–¹æ³•*
        # è¿™é‡Œæ¼”ç¤ºç›´æ¥è°ƒç”¨ openai client (å¦‚æœä½ åœ¨ controller æš´éœ²äº† client)
        # æˆ–è€…æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ get_completion

        response_text = self.memory_system.llm_controller.llm.client.chat.completions.create(
            model=self.memory_system.llm_controller.llm.model,
            messages=messages,
            temperature=0.7
        ).choices[0].message.content

        # --- æ­¥éª¤ 4: æ›´æ–°çŸ­æœŸè®°å¿† ---
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": response_text})

        return response_text

    def save_current_interaction(self, user_input: str, response: str):
        """
        å¯é€‰ï¼šå°†åˆšæ‰çš„å¯¹è¯ä¹Ÿå­˜å…¥é•¿æœŸè®°å¿†åº“
        è¿™æ ·æœºå™¨äººå°±èƒ½è®°ä½'å®ƒåˆšæ‰å’Œä½ è¯´è¿‡ä»€ä¹ˆ'ï¼Œå®ç°è‡ªæˆ‘è¿›åŒ–çš„è®°å¿†
        """
        # å¹¶ä¸æ˜¯æ¯å¥è¯éƒ½å€¼å¾—å­˜ï¼Œå¯ä»¥åŠ ä¸€ä¸ªåˆ¤æ–­é€»è¾‘ï¼Œæˆ–è€…ç›´æ¥å­˜
        content_to_save = f"User asked: {user_input}\nAssistant answered: {response}"
        self.memory_system.add_note(content_to_save, tags=["chat_history"])